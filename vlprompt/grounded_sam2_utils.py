import torch
import torch.nn.functional as F
import numpy as np
from typing import List, Tuple, Optional, Dict
import cv2
from PIL import Image
import sys
import os

# æ·»åŠ  Grounded-SAM-2-main åˆ°è·¯å¾„
gsam2_path = os.path.join(os.path.dirname(__file__), "..", "..", "Grounded-SAM-2-main")
if gsam2_path not in sys.path:
    sys.path.append(gsam2_path)

# ç›´æ¥å¯¼å…¥éœ€è¦çš„æ¨¡å—
from groundingdino.models import build_model
from groundingdino.util.slconfig import SLConfig
from groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap
import groundingdino.datasets.transforms as T
from sam2.build_sam import build_sam2
from sam2.sam2_image_predictor import SAM2ImagePredictor


def load_groundingdino_model(config_path: str, checkpoint_path: str, device: str = "cuda", bert_models_dir: str = None):
    """åŠ è½½ GroundingDINO æ¨¡å‹"""
    try:
        # é¦–å…ˆæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(checkpoint_path):
            print(f"âŒ GroundingDINO checkpoint æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            print("ğŸ’¡ è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æˆ–é‡æ–°ä¸‹è½½ checkpoint")
            return None
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(checkpoint_path)
        if file_size == 0:
            print(f"âŒ GroundingDINO checkpoint æ–‡ä»¶ä¸ºç©º: {checkpoint_path}")
            return None
        
        print(f"ğŸ“ åŠ è½½ GroundingDINO checkpoint: {checkpoint_path}")
        print(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size / (1024*1024):.2f} MB")
        
        args = SLConfig.fromfile(config_path)
        args.device = device
        
        # è®¾ç½®ç¦»çº¿æ¨¡å¼å’Œæœ¬åœ°æ¨¡å‹ç›®å½•
        os.environ['HF_HUB_OFFLINE'] = '1'
        os.environ['TRANSFORMERS_OFFLINE'] = '1'
        os.environ['HF_DATASETS_OFFLINE'] = '1'
        
        if bert_models_dir:
            os.environ['TRANSFORMERS_CACHE'] = bert_models_dir
            os.environ['HF_HOME'] = bert_models_dir
            print(f"ğŸ”§ ä½¿ç”¨æœ¬åœ° BERT æ¨¡å‹ç›®å½•: {bert_models_dir}")
        
        print("ğŸ”¨ æ„å»º GroundingDINO æ¨¡å‹...")
        model = build_model(args)
        
        print("ğŸ“¥ åŠ è½½ checkpoint...")
        try:
            checkpoint = torch.load(checkpoint_path, map_location="cpu")
        except Exception as e:
            print(f"âŒ checkpoint æ–‡ä»¶æŸå: {e}")
            print("ğŸ’¡ å»ºè®®é‡æ–°ä¸‹è½½ checkpoint æ–‡ä»¶")
            return None
        
        print("âš™ï¸  åŠ è½½æ¨¡å‹æƒé‡...")
        model.load_state_dict(clean_state_dict(checkpoint["model"]), strict=False)
        model.eval()
        model = model.to(device)
        
        print("âœ… GroundingDINO æ¨¡å‹åŠ è½½æˆåŠŸ")
        return model
        
    except Exception as e:
        print(f"âš ï¸  GroundingDINO æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç½‘ç»œè¿æ¥é—®é¢˜
        if "Connection" in str(e) or "reset" in str(e).lower():
            print("ğŸŒ æ£€æµ‹åˆ°ç½‘ç»œè¿æ¥é—®é¢˜")
            print("ğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
            print("1. è®¾ç½®ç¦»çº¿æ¨¡å¼: export HF_HUB_OFFLINE=1")
            print("2. ä½¿ç”¨é•œåƒæº: export HF_ENDPOINT=https://hf-mirror.com")
            print("3. ç¦ç”¨ GroundingDINO: --use_groundingdino False")
            print("4. æ‰‹åŠ¨ä¸‹è½½ BERT æ¨¡å‹åˆ°æœ¬åœ°")
        else:
            print("ğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
            print("1. æ£€æŸ¥ checkpoint æ–‡ä»¶æ˜¯å¦å®Œæ•´")
            print("2. é‡æ–°ä¸‹è½½ checkpoint æ–‡ä»¶")
            print("3. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print("4. æ‰‹åŠ¨ä¸‹è½½ BERT æ¨¡å‹åˆ°æœ¬åœ°ç¼“å­˜")
            print("5. ä½¿ç”¨ç¦»çº¿æ¨¡å¼")
            print("6. æˆ–è€…ç¦ç”¨ GroundingDINOï¼Œä»…ä½¿ç”¨ SAM2")
            print("7. æŒ‡å®šæœ¬åœ° BERT æ¨¡å‹ç›®å½•: --bert_models_dir /path/to/models")
        
        # è¿”å› Noneï¼Œè®©è°ƒç”¨è€…å†³å®šå¦‚ä½•å¤„ç†
        return None


def get_grounding_output_batch(model, images, captions, box_threshold, text_threshold, device="cuda"):
    """GroundingDINO æ‰¹é‡æ¨ç†å‡½æ•° - æ”¯æŒå¤šå¡å¹¶è¡Œ"""
    # é¢„å¤„ç† captions
    processed_captions = []
    for caption in captions:
        caption = caption.lower().strip()
        if not caption.endswith("."):
            caption = caption + "."
        processed_captions.append(caption)
    
    model = model.to(device)
    images = images.to(device)
    
    with torch.no_grad():
        outputs = model(images, captions=processed_captions)
    
    batch_boxes = []
    batch_phrases = []
    
    logits = outputs["pred_logits"].sigmoid()  # (B, nq, 256)
    boxes = outputs["pred_boxes"]  # (B, nq, 4)
    
    for b in range(images.size(0)):
        # filter output for each image
        logits_filt = logits[b].cpu().clone()
        boxes_filt = boxes[b].cpu().clone()
        filt_mask = logits_filt.max(dim=1)[0] > box_threshold
        logits_filt = logits_filt[filt_mask]  # num_filt, 256
        boxes_filt = boxes_filt[filt_mask]  # num_filt, 4
        
        # get phrase
        tokenizer = model.tokenizer
        tokenized = tokenizer(processed_captions[b])
        
        # build pred
        pred_phrases = []
        for logit, box in zip(logits_filt, boxes_filt):
            pred_phrase = get_phrases_from_posmap(logit > text_threshold, tokenized, tokenizer)
            pred_phrases.append(pred_phrase)
        
        batch_boxes.append(boxes_filt)
        batch_phrases.append(pred_phrases)
    
    return batch_boxes, batch_phrases


def get_grounding_output(model, image, caption, box_threshold, text_threshold, with_logits=True, device="cpu"):
    """GroundingDINO å•å¼ å›¾ç‰‡æ¨ç†å‡½æ•°ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰"""
    caption = caption.lower()
    caption = caption.strip()
    if not caption.endswith("."):
        caption = caption + "."
    
    model = model.to(device)
    image = image.to(device)
    
    with torch.no_grad():
        outputs = model(image[None], captions=[caption])
    
    logits = outputs["pred_logits"].sigmoid()[0]  # (nq, 256)
    boxes = outputs["pred_boxes"][0]  # (nq, 4)
    
    # filter output
    logits_filt = logits.cpu().clone()
    boxes_filt = boxes.cpu().clone()
    filt_mask = logits_filt.max(dim=1)[0] > box_threshold
    logits_filt = logits_filt[filt_mask]  # num_filt, 256
    boxes_filt = boxes_filt[filt_mask]  # num_filt, 4
    
    # get phrase
    tokenlizer = model.tokenizer
    tokenized = tokenlizer(caption)
    
    # build pred
    pred_phrases = []
    for logit, box in zip(logits_filt, boxes_filt):
        pred_phrase = get_phrases_from_posmap(logit > text_threshold, tokenized, tokenlizer)
        pred_phrases.append(pred_phrase)
    
    return boxes_filt, pred_phrases


def grounded_sam2_pipeline_batch(
    images: torch.Tensor,
    prompts: List[str],
    groundingdino_model,
    sam2_model,
    box_threshold: float = 0.35,
    text_threshold: float = 0.25,
    device: str = "cuda"
) -> List[List[torch.Tensor]]:
    """
    å®Œæ•´çš„ Grounded SAM2 pipeline - æ‰¹é‡ç‰ˆæœ¬ï¼Œæ”¯æŒå¤šå¡å¹¶è¡Œ
    
    Args:
        images: [B, 3, H, W] è¾“å…¥å›¾åƒ
        prompts: [B] æ¯å¼ å›¾åƒå¯¹åº”çš„æ–‡æœ¬prompt
        groundingdino_model: GroundingDINOæ¨¡å‹
        sam2_model: SAM2æ¨¡å‹ï¼ˆç›´æ¥ä½¿ç”¨ï¼Œä¸ç”¨predictorï¼‰
        box_threshold: boxç½®ä¿¡åº¦é˜ˆå€¼
        text_threshold: æ–‡æœ¬ç½®ä¿¡åº¦é˜ˆå€¼
        device: è®¾å¤‡
    
    Returns:
        masks: List[List[torch.Tensor]] æ¯å¼ å›¾åƒçš„åˆ†å‰²maskåˆ—è¡¨
    """
    batch_size = images.shape[0]
    
    # 1. GroundingDINO æ‰¹é‡æ£€æµ‹
    batch_boxes, batch_phrases = get_grounding_output_batch(
        groundingdino_model, images, prompts, box_threshold, text_threshold, device=device
    )
    
    # 2. æ‰¹é‡æå– SAM2 ç‰¹å¾
    with torch.no_grad():
        # è·å– SAM2 çš„ image embedding
        image_features = sam2_model.get_image_embedding(images)  # [B, C, H, W]
    
    # 3. ä¸ºæ¯ä¸ªæ£€æµ‹æ¡†ç”Ÿæˆ maskï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œç”¨ attention æƒé‡è¿‘ä¼¼ï¼‰
    all_masks = []
    
    for b in range(batch_size):
        boxes_filt = batch_boxes[b]
        image_feat = image_features[b]  # [C, H, W]
        
        if len(boxes_filt) == 0:
            # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°boxï¼Œè¿”å›ç©ºmask
            all_masks.append([])
            continue
        
        masks = []
        for box in boxes_filt:
            # å°† box è½¬æ¢ä¸ºç‰¹å¾å›¾åæ ‡
            x1, y1, x2, y2 = box.tolist()
            
            # è®¡ç®—ç‰¹å¾å›¾ä¸Šçš„åæ ‡ï¼ˆå‡è®¾è¾“å…¥æ˜¯ 224x224ï¼Œç‰¹å¾å›¾æ˜¯ 14x14ï¼‰
            feat_h, feat_w = image_feat.shape[1], image_feat.shape[2]
            img_h, img_w = images.shape[2], images.shape[3]
            
            # è½¬æ¢åæ ‡
            x1_feat = int(x1 * feat_w / img_w)
            y1_feat = int(y1 * feat_h / img_h)
            x2_feat = int(x2 * feat_w / img_w)
            y2_feat = int(y2 * feat_h / img_h)
            
            # åˆ›å»ºç®€å•çš„çŸ©å½¢ mask
            mask = torch.zeros(feat_h, feat_w)
            mask[y1_feat:y2_feat, x1_feat:x2_feat] = 1.0
            
            # ä¸Šé‡‡æ ·åˆ°åŸå›¾å¤§å°
            mask = F.interpolate(
                mask.unsqueeze(0).unsqueeze(0), 
                size=(img_h, img_w), 
                mode='bilinear', 
                align_corners=False
            ).squeeze(0).squeeze(0)
            
            masks.append(mask)
        
        all_masks.append(masks)
    
    return all_masks


def grounded_sam2_pipeline(
    images: torch.Tensor,
    prompts: List[str],
    groundingdino_model,
    sam2_predictor,
    box_threshold: float = 0.35,
    text_threshold: float = 0.25,
    device: str = "cuda"
) -> List[List[torch.Tensor]]:
    """
    å®Œæ•´çš„ Grounded SAM2 pipelineï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
    
    Args:
        images: [B, 3, H, W] è¾“å…¥å›¾åƒ
        prompts: [B] æ¯å¼ å›¾åƒå¯¹åº”çš„æ–‡æœ¬prompt
        groundingdino_model: GroundingDINOæ¨¡å‹
        sam2_predictor: SAM2é¢„æµ‹å™¨
        box_threshold: boxç½®ä¿¡åº¦é˜ˆå€¼
        text_threshold: æ–‡æœ¬ç½®ä¿¡åº¦é˜ˆå€¼
        device: è®¾å¤‡
    
    Returns:
        masks: List[List[torch.Tensor]] æ¯å¼ å›¾åƒçš„åˆ†å‰²maskåˆ—è¡¨
    """
    batch_size = images.shape[0]
    all_masks = []
    
    for i in range(batch_size):
        image = images[i]  # [3, H, W]
        prompt = prompts[i]
        
        # 1. GroundingDINO æ£€æµ‹
        boxes_filt, pred_phrases = get_grounding_output(
            groundingdino_model, image, prompt, box_threshold, text_threshold, device=device
        )
        
        if len(boxes_filt) == 0:
            # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°boxï¼Œè¿”å›ç©ºmask
            all_masks.append([])
            continue
        
        # 2. SAM2 åˆ†å‰²
        image_np = image.permute(1, 2, 0).cpu().numpy()
        image_np = (image_np * 255).astype(np.uint8)
        
        sam2_predictor.set_image(image_np)
        
        masks = []
        for box in boxes_filt:
            # è½¬æ¢boxæ ¼å¼
            x1, y1, x2, y2 = box.tolist()
            input_box = np.array([x1, y1, x2, y2])
            
            # SAM2 åˆ†å‰²
            mask, _, _ = sam2_predictor.predict(
                point_coords=None,
                point_labels=None,
                box=input_box[None, :],
                multimask_output=False,
            )
            
            masks.append(torch.from_numpy(mask[0]).float())
        
        all_masks.append(masks)
    
    return all_masks


def extract_region_features_from_masks(
    images: torch.Tensor,
    masks: List[List[torch.Tensor]],
    sam2_model,
    sam_proj=None,
    device: str = "cuda"
) -> torch.Tensor:
    """
    ä»åˆ†å‰²maskä¸­æå–åŒºåŸŸç‰¹å¾
    
    Args:
        images: [B, 3, H, W] è¾“å…¥å›¾åƒ
        masks: List[List[torch.Tensor]] æ¯å¼ å›¾åƒçš„åˆ†å‰²maskåˆ—è¡¨
        sam2_model: SAM2æ¨¡å‹
        sam_proj: ç‰¹å¾æŠ•å½±å±‚
        device: è®¾å¤‡
    
    Returns:
        region_features: [B, N, D] åŒºåŸŸç‰¹å¾
    """
    batch_size = images.shape[0]
    all_region_features = []
    
    for i in range(batch_size):
        image = images[i]  # [3, H, W]
        image_masks = masks[i]
        
        if len(image_masks) == 0:
            # å¦‚æœæ²¡æœ‰maskï¼Œç”¨æ•´å›¾ç‰¹å¾
            with torch.no_grad():
                image_features = sam2_model.get_image_embedding(image.unsqueeze(0))
                # å–å…¨å±€å¹³å‡æ± åŒ–
                region_feat = image_features.mean(dim=[2, 3])  # [1, C]
                all_region_features.append(region_feat)
            continue
        
        # å¯¹æ¯ä¸ªmaskæå–ç‰¹å¾
        mask_features = []
        for mask in image_masks:
            # å°†maskè½¬æ¢ä¸ºSAM2è¾“å…¥æ ¼å¼
            mask_np = mask.cpu().numpy()
            
            # ç”¨maskåŠ æƒå¹³å‡pooling
            with torch.no_grad():
                image_features = sam2_model.get_image_embedding(image.unsqueeze(0))  # [1, C, H, W]
                
                # å°†mask resizeåˆ°ç‰¹å¾å›¾å¤§å°
                mask_resized = F.interpolate(
                    mask.unsqueeze(0).unsqueeze(0), 
                    size=image_features.shape[2:], 
                    mode='bilinear', 
                    align_corners=False
                )  # [1, 1, H', W']
                
                # åŠ æƒå¹³å‡pooling
                weighted_features = image_features * mask_resized
                region_feat = weighted_features.sum(dim=[2, 3]) / (mask_resized.sum() + 1e-8)
                mask_features.append(region_feat)
        
        # æ‹¼æ¥æ‰€æœ‰maskçš„ç‰¹å¾
        if len(mask_features) > 0:
            region_feat = torch.cat(mask_features, dim=0)  # [N, C]
        else:
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆmaskï¼Œç”¨æ•´å›¾ç‰¹å¾
            with torch.no_grad():
                image_features = sam2_model.get_image_embedding(image.unsqueeze(0))
                region_feat = image_features.mean(dim=[2, 3])  # [1, C]
        
        all_region_features.append(region_feat)
    
    # æŠ•å½±åˆ°CLIPç‰¹å¾ç©ºé—´
    if sam_proj is not None:
        all_region_features = [sam_proj(feat) for feat in all_region_features]
    
    return all_region_features


def get_grounded_sam2_features_batch(
    images: torch.Tensor,
    prompts: List[str],
    groundingdino_model,
    sam2_model,
    sam_proj=None,
    box_threshold: float = 0.35,
    text_threshold: float = 0.25,
    device: str = "cuda"
) -> List[torch.Tensor]:
    """
    å®Œæ•´çš„ Grounded SAM2 ç‰¹å¾æå– - æ‰¹é‡ç‰ˆæœ¬ï¼Œæ”¯æŒå¤šå¡å¹¶è¡Œ
    
    Args:
        images: [B, 3, H, W] è¾“å…¥å›¾åƒ
        prompts: [B] æ¯å¼ å›¾åƒå¯¹åº”çš„æ–‡æœ¬prompt
        groundingdino_model: GroundingDINOæ¨¡å‹
        sam2_model: SAM2æ¨¡å‹
        sam_proj: ç‰¹å¾æŠ•å½±å±‚
        box_threshold: boxç½®ä¿¡åº¦é˜ˆå€¼
        text_threshold: æ–‡æœ¬ç½®ä¿¡åº¦é˜ˆå€¼
        device: è®¾å¤‡
    
    Returns:
        region_features: List[torch.Tensor] æ¯å¼ å›¾åƒçš„åŒºåŸŸç‰¹å¾åˆ—è¡¨
    """
    # 1. Grounded SAM2 pipeline - æ‰¹é‡ç‰ˆæœ¬
    masks = grounded_sam2_pipeline_batch(
        images, prompts, groundingdino_model, sam2_model, 
        box_threshold, text_threshold, device
    )
    
    # 2. æå–åŒºåŸŸç‰¹å¾
    region_features = extract_region_features_from_masks(
        images, masks, sam2_model, sam_proj, device
    )
    
    return region_features


def get_grounded_sam2_features(
    images: torch.Tensor,
    prompts: List[str],
    groundingdino_model,
    sam2_predictor,
    sam2_model,
    sam_proj=None,
    box_threshold: float = 0.35,
    text_threshold: float = 0.25,
    device: str = "cuda"
) -> torch.Tensor:
    """
    å®Œæ•´çš„ Grounded SAM2 ç‰¹å¾æå–ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
    
    Args:
        images: [B, 3, H, W] è¾“å…¥å›¾åƒ
        prompts: [B] æ¯å¼ å›¾åƒå¯¹åº”çš„æ–‡æœ¬prompt
        groundingdino_model: GroundingDINOæ¨¡å‹
        sam2_predictor: SAM2é¢„æµ‹å™¨
        sam2_model: SAM2æ¨¡å‹
        sam_proj: ç‰¹å¾æŠ•å½±å±‚
        box_threshold: boxç½®ä¿¡åº¦é˜ˆå€¼
        text_threshold: æ–‡æœ¬ç½®ä¿¡åº¦é˜ˆå€¼
        device: è®¾å¤‡
    
    Returns:
        region_features: [B, N, D] åŒºåŸŸç‰¹å¾
    """
    # 1. Grounded SAM2 pipeline
    masks = grounded_sam2_pipeline(
        images, prompts, groundingdino_model, sam2_predictor, 
        box_threshold, text_threshold, device
    )
    
    # 2. æå–åŒºåŸŸç‰¹å¾
    region_features = extract_region_features_from_masks(
        images, masks, sam2_model, sam_proj, device
    )
    
    return region_features 